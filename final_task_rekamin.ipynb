{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPwZhox9VE2oqX6Bx7Wr1O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annisafitribas/ft_credit_home/blob/main/final_task_rekamin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "j60Zb0jZsB6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uECHRylKjz2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Home Credit — Full end-to-end pipeline with visuals\n",
        "Run in Google Colab (recommended)\n",
        "Produces: models, submissions, PNG visualizations, and a 10-slide PPTX including visuals.\n",
        "\"\"\"\n",
        "\n",
        "# 0. Setup\n",
        "import os, sys, time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "WORKDIR = '/content/home_credit_task'\n",
        "os.makedirs(WORKDIR, exist_ok=True)\n",
        "print(\"WORKDIR:\", WORKDIR)\n",
        "\n",
        "# install dependencies if missing (Colab-friendly)\n",
        "try:\n",
        "    import gdown\n",
        "except Exception:\n",
        "    !pip install -q gdown\n",
        "    import gdown\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception:\n",
        "    !pip install -q lightgbm\n",
        "    import lightgbm as lgb\n",
        "\n",
        "try:\n",
        "    from pptx import Presentation\n",
        "    from pptx.util import Inches, Pt\n",
        "except Exception:\n",
        "    !pip install -q python-pptx\n",
        "    from pptx import Presentation\n",
        "    from pptx.util import Inches, Pt\n",
        "\n",
        "try:\n",
        "    import matplotlib\n",
        "    import matplotlib.pyplot as plt\n",
        "except Exception:\n",
        "    !pip install -q matplotlib\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn / joblib\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
        "from sklearn.impute import SimpleImputer\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Download dataset files (IDs provided)\n",
        "# -----------------------------\n",
        "files = {\n",
        "    'application_train.csv': '1q059QolR6CNxB0PWESAjkEWIprNutajA',\n",
        "    'application_test.csv' : '1QD7ehk_hzXze0vHQuYa5qyqfDcfI8Sex',\n",
        "    'bureau.csv'           : '1hndizX1t5ab0DTnKMTedqVJ1ZxLVclhF',\n",
        "    'bureau_balance.csv'   : '1OXEQb_L6S_mZALJi4--C6RyFI6yOsq4x',\n",
        "    'credit_card_balance.csv': '1t6Hhsmj0vSCCKUlNXht_xDQ6Z6l4M0Vu',\n",
        "    'installments_payments.csv': '126xrKCW5EQrxkQoDwmN-yb00ILBKnhR8',\n",
        "    'POS_CASH_balance.csv' : '1dODAmBQLaylpM2JcCHfc4KNbbtKY7xhA',\n",
        "    'previous_application.csv': '1D4O7xf-lF_3oBeu6XMwzhpXtSvhcgoBU',\n",
        "    'HomeCredit_columns_description.csv': '1v2iGGOJjlUGSTsQz-bsjtjtyM5IQp7uW',\n",
        "    'sample_submission.csv': '1JongVA9fWMYml5XKVnbhm8TUlR5Efs0n'\n",
        "}\n",
        "\n",
        "for fname, fid in files.items():\n",
        "    dest = os.path.join(WORKDIR, fname)\n",
        "    if not os.path.exists(dest):\n",
        "        print(\"Downloading\", fname)\n",
        "        url = f\"https://drive.google.com/uc?export=download&id={fid}\"\n",
        "        gdown.download(url, dest, quiet=False)\n",
        "    else:\n",
        "        print(\"Exists:\", fname)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Load CSVs\n",
        "# -----------------------------\n",
        "print(\"\\nLoading CSVs ...\")\n",
        "train = pd.read_csv(os.path.join(WORKDIR, 'application_train.csv'), low_memory=False)\n",
        "test  = pd.read_csv(os.path.join(WORKDIR, 'application_test.csv'), low_memory=False)\n",
        "bureau = pd.read_csv(os.path.join(WORKDIR, 'bureau.csv'), low_memory=False)\n",
        "bureau_balance = pd.read_csv(os.path.join(WORKDIR, 'bureau_balance.csv'), low_memory=False)\n",
        "credit_card_balance = pd.read_csv(os.path.join(WORKDIR, 'credit_card_balance.csv'), low_memory=False)\n",
        "installments = pd.read_csv(os.path.join(WORKDIR, 'installments_payments.csv'), low_memory=False)\n",
        "pos_cash = pd.read_csv(os.path.join(WORKDIR, 'POS_CASH_balance.csv'), low_memory=False)\n",
        "prev_app = pd.read_csv(os.path.join(WORKDIR, 'previous_application.csv'), low_memory=False)\n",
        "sample_sub = pd.read_csv(os.path.join(WORKDIR, 'sample_submission.csv'), low_memory=False)\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(\"train\", train.shape, \"test\", test.shape)\n",
        "print(\"bureau\", bureau.shape, \"bureau_balance\", bureau_balance.shape)\n",
        "print(\"credit_card_balance\", credit_card_balance.shape, \"installments\", installments.shape)\n",
        "print(\"pos_cash\", pos_cash.shape, \"previous_application\", prev_app.shape)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Quick EDA & visuals\n",
        "# -----------------------------\n",
        "print(\"\\n--- TARGET distribution ---\")\n",
        "print(train['TARGET'].value_counts(normalize=True))\n",
        "\n",
        "# helper for saving plots\n",
        "def savefig(fig, filename):\n",
        "    path = os.path.join(WORKDIR, filename)\n",
        "    fig.savefig(path, bbox_inches='tight')\n",
        "    print('Saved plot:', path)\n",
        "\n",
        "# 3.1 target distribution plot\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "ax = fig.add_subplot(111)\n",
        "counts = train['TARGET'].value_counts().sort_index()\n",
        "ax.bar(counts.index.astype(str), counts.values)\n",
        "ax.set_title('Target distribution (counts)')\n",
        "ax.set_xlabel('TARGET')\n",
        "ax.set_ylabel('Count')\n",
        "savefig(fig, 'target_distribution.png')\n",
        "plt.close(fig)\n",
        "\n",
        "# 3.2 top missing features (bar)\n",
        "missing = train.isna().mean().sort_values(ascending=False).head(30)\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.barh(missing.index[::-1], missing.values[::-1])\n",
        "ax.set_title('Top missing percentage (train)')\n",
        "ax.set_xlabel('Fraction missing')\n",
        "savefig(fig, 'missing_pct_top.png')\n",
        "plt.close(fig)\n",
        "\n",
        "# 3.3 correlation heatmap of numeric features (sampled for speed)\n",
        "num = train.select_dtypes(include=[np.number]).drop(['SK_ID_CURR','TARGET'], axis=1, errors='ignore')\n",
        "# sample columns to avoid huge matrix\n",
        "num_small = num.sample(n=min(30, num.shape[1]), axis=1)\n",
        "corr = num_small.corr()\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.imshow(corr.values, interpolation='nearest')\n",
        "ax.set_xticks(np.arange(len(corr.columns)))\n",
        "ax.set_xticklabels(corr.columns, rotation=90, fontsize=8)\n",
        "ax.set_yticks(np.arange(len(corr.columns)))\n",
        "ax.set_yticklabels(corr.columns, fontsize=8)\n",
        "ax.set_title('Correlation matrix (subset)')\n",
        "fig.colorbar(cax, ax=ax)\n",
        "savefig(fig, 'corr_matrix_subset.png')\n",
        "plt.close(fig)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Feature engineering (same as baseline but kept clear)\n",
        "# -----------------------------\n",
        "# 4.1 Aggregations from bureau (per SK_ID_CURR)\n",
        "b_agg = bureau.groupby('SK_ID_CURR').agg(\n",
        "    bureau_loans_count = ('SK_ID_BUREAU', 'count'),\n",
        "    bureau_credit_sum_mean = ('AMT_CREDIT_SUM', 'mean'),\n",
        "    bureau_credit_sum_max = ('AMT_CREDIT_SUM', 'max'),\n",
        "    bureau_active_cnt = ('CREDIT_ACTIVE', lambda x: (x=='Active').sum())\n",
        ").reset_index()\n",
        "\n",
        "# 4.2 bureau_balance -> bad rate per bureau id then agg\n",
        "bb_bad = bureau_balance[bureau_balance['STATUS'].isin(['2','3','4','5'])].groupby('SK_ID_BUREAU').size().rename('bad_months')\n",
        "bb_tot = bureau_balance.groupby('SK_ID_BUREAU').size().rename('total_months')\n",
        "bb = pd.concat([bb_bad, bb_tot], axis=1).fillna(0)\n",
        "bb['bad_rate'] = bb['bad_months'] / bb['total_months']\n",
        "bureau2 = bureau.merge(bb.reset_index(), on='SK_ID_BUREAU', how='left')\n",
        "b2_agg = bureau2.groupby('SK_ID_CURR').agg(\n",
        "    bureau_prev_bad_rate_mean = ('bad_rate','mean'),\n",
        "    bureau_prev_months_mean = ('total_months','mean')\n",
        ").reset_index()\n",
        "\n",
        "b_agg = b_agg.merge(b2_agg, on='SK_ID_CURR', how='left')\n",
        "\n",
        "# 4.3 previous_application aggregates\n",
        "prev_agg = prev_app.groupby('SK_ID_CURR').agg(\n",
        "    prev_count = ('SK_ID_PREV','count'),\n",
        "    prev_amt_app_mean = ('AMT_APPLICATION','mean'),\n",
        "    prev_amt_credit_mean = ('AMT_CREDIT','mean'),\n",
        "    prev_approved = ('NAME_CONTRACT_STATUS', lambda x: (x=='Approved').sum())\n",
        ").reset_index()\n",
        "\n",
        "# 4.4 installments\n",
        "inst_agg = installments.groupby('SK_ID_CURR').agg(\n",
        "    inst_count = ('NUM_INSTALMENT_VERSION','count'),\n",
        "    inst_amt_sum = ('AMT_PAYMENT','sum'),\n",
        "    inst_delay_mean = ('DAYS_ENTRY_PAYMENT', lambda x: np.nanmean(x - installments.loc[x.index,'DAYS_INSTALMENT']))\n",
        ").reset_index()\n",
        "\n",
        "# 4.5 credit_card & pos\n",
        "cc_agg = credit_card_balance.groupby('SK_ID_CURR').agg(\n",
        "    cc_count = ('SK_ID_PREV','count'),\n",
        "    cc_bal_mean = ('AMT_BALANCE','mean'),\n",
        "    cc_limit_mean = ('AMT_CREDIT_LIMIT_ACTUAL','mean')\n",
        ").reset_index()\n",
        "\n",
        "pos_agg = pos_cash.groupby('SK_ID_CURR').agg(\n",
        "    pos_count = ('SK_ID_PREV','count'),\n",
        "    pos_dpd_mean = ('SK_DPD','mean')\n",
        ").reset_index()\n",
        "\n",
        "# 4.6 application-level features\n",
        "def make_app_features(df):\n",
        "    df = df.copy()\n",
        "    df['DAYS_BIRTH_YEARS'] = (-df['DAYS_BIRTH']) / 365.25\n",
        "    df['DAYS_EMPLOYED_YEARS'] = df['DAYS_EMPLOYED'].replace(365243, np.nan) / -365.25\n",
        "    df['INCOME_CREDIT_RATIO'] = df['AMT_INCOME_TOTAL'] / (df['AMT_CREDIT'] + 1)\n",
        "    df['CREDIT_GOODS_RATIO'] = df['AMT_CREDIT'] / (df['AMT_GOODS_PRICE'] + 1)\n",
        "    return df[['SK_ID_CURR','DAYS_BIRTH_YEARS','DAYS_EMPLOYED_YEARS','AMT_INCOME_TOTAL','AMT_CREDIT','INCOME_CREDIT_RATIO','CREDIT_GOODS_RATIO']]\n",
        "\n",
        "app_train_feats = make_app_features(train)\n",
        "app_test_feats = make_app_features(test)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Merge features\n",
        "# -----------------------------\n",
        "train_base = train[['SK_ID_CURR','TARGET']].merge(b_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(prev_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(inst_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(cc_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(pos_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(app_train_feats, on='SK_ID_CURR', how='left')\n",
        "\n",
        "test_base = test[['SK_ID_CURR']].merge(b_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(prev_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(inst_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(cc_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(pos_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(app_test_feats, on='SK_ID_CURR', how='left')\n",
        "\n",
        "print(\"\\nMerged shapes:\", train_base.shape, test_base.shape)\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Prepare X, y; preprocessing\n",
        "# -----------------------------\n",
        "Y = train_base['TARGET']\n",
        "X = train_base.drop(['SK_ID_CURR','TARGET'], axis=1)\n",
        "X_test = test_base.drop(['SK_ID_CURR'], axis=1)\n",
        "\n",
        "# numeric / categorical separation\n",
        "num_cols = [c for c in X.columns if X[c].dtype.kind in 'biufc']\n",
        "cat_cols = [c for c in X.columns if c not in num_cols]\n",
        "\n",
        "# Impute numeric with median (computed from train)\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
        "X_test[num_cols] = num_imputer.transform(X_test[num_cols])\n",
        "\n",
        "# For categorical (if any), fill and label-encode simple\n",
        "for c in cat_cols:\n",
        "    X[c] = X[c].fillna('MISSING').astype(str)\n",
        "    X_test[c] = X_test[c].fillna('MISSING').astype(str)\n",
        "for c in cat_cols:\n",
        "    X[c], _ = pd.factorize(X[c])\n",
        "    X_test[c], _ = pd.factorize(X_test[c])\n",
        "\n",
        "# Ensure X_test has all columns (reindex)\n",
        "X_test = X_test.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "print(\"\\nNumber of features:\", X.shape[1])\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Train/validation split and scaling\n",
        "# -----------------------------\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
        "scaler = StandardScaler()\n",
        "X_tr_scaled = scaler.fit_transform(X_tr)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# -----------------------------\n",
        "# 8. Logistic Regression\n",
        "# -----------------------------\n",
        "print(\"\\n=== Logistic Regression ===\")\n",
        "lr = LogisticRegression(max_iter=2000, class_weight='balanced', n_jobs=-1)\n",
        "# 5-fold CV\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "lr_cv_scores = []\n",
        "for tr_idx, vc_idx in kf.split(X, Y):\n",
        "    lr.fit(scaler.fit_transform(X.iloc[tr_idx]), Y.iloc[tr_idx])\n",
        "    p = lr.predict_proba(scaler.transform(X.iloc[vc_idx]))[:,1]\n",
        "    lr_cv_scores.append(roc_auc_score(Y.iloc[vc_idx], p))\n",
        "print(\"LR 5-fold AUC: %.5f ± %.5f\" % (np.mean(lr_cv_scores), np.std(lr_cv_scores)))\n",
        "\n",
        "# fit on training partition and validate\n",
        "lr.fit(X_tr_scaled, y_tr)\n",
        "proba_lr_val = lr.predict_proba(X_val_scaled)[:,1]\n",
        "print(\"LR holdout AUC:\", roc_auc_score(y_val, proba_lr_val))\n",
        "print(\"LR classification report (holdout, threshold=0.5):\")\n",
        "print(classification_report(y_val, (proba_lr_val>0.5).astype(int)))\n",
        "\n",
        "# save logistic model & scaler\n",
        "joblib.dump(lr, os.path.join(WORKDIR, 'model_logistic.pkl'))\n",
        "joblib.dump(scaler, os.path.join(WORKDIR, 'scaler_logistic.pkl'))\n",
        "\n",
        "# ROC curve plot (LR)\n",
        "fpr, tpr, _ = roc_curve(y_val, proba_lr_val)\n",
        "roc_auc_lr = auc(fpr, tpr)\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(fpr, tpr)\n",
        "ax.plot([0,1],[0,1], linestyle='--')\n",
        "ax.set_title(f'Logistic ROC (AUC={roc_auc_lr:.4f})')\n",
        "ax.set_xlabel('FPR')\n",
        "ax.set_ylabel('TPR')\n",
        "savefig(fig, 'roc_logistic.png')\n",
        "plt.close(fig)\n",
        "\n",
        "# -----------------------------\n",
        "# 9. LightGBM with RandomizedSearchCV\n",
        "# -----------------------------\n",
        "print(\"\\nLightGBM\")\n",
        "\n",
        "best_lgb = lgb.LGBMClassifier(\n",
        "    objective='binary',\n",
        "    random_state=42,\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=400,\n",
        "    num_leaves=63,\n",
        "    min_child_samples=20,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "best_lgb.fit(X_tr, y_tr)\n",
        "proba_lgb_val = best_lgb.predict_proba(X_val)[:,1]\n",
        "\n",
        "print(\"LightGBM FAST holdout AUC:\", roc_auc_score(y_val, proba_lgb_val))\n",
        "print(\"LGB classification report (holdout):\")\n",
        "print(classification_report(y_val, (proba_lgb_val > 0.5).astype(int)))\n",
        "\n",
        "# Save model\n",
        "joblib.dump(best_lgb, os.path.join(WORKDIR, 'model_lgb.pkl'))\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 10. Feature importance (LGB) and plot\n",
        "# -----------------------------\n",
        "fi = pd.Series(best_lgb.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "print('\\nTop 20 features (LightGBM):')\n",
        "print(fi.head(20))\n",
        "\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "ax = fig.add_subplot(111)\n",
        "topn = fi.head(20)[::-1]\n",
        "ax.barh(topn.index, topn.values)\n",
        "ax.set_title('Top 20 feature importances (LightGBM)')\n",
        "ax.set_xlabel('Importance')\n",
        "savefig(fig, 'feature_importance_top20.png')\n",
        "plt.close(fig)\n",
        "\n",
        "# -----------------------------\n",
        "# 11. Create submissions\n",
        "# -----------------------------\n",
        "print('\\nCreating submissions ...')\n",
        "preds_lg_test = best_lgb.predict_proba(X_test)[:,1]\n",
        "preds_lr_test = lr.predict_proba(X_test_scaled)[:,1]\n",
        "\n",
        "sub_lgb = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': preds_lg_test})\n",
        "sub_lr  = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': preds_lr_test})\n",
        "\n",
        "sub_lgb.to_csv(os.path.join(WORKDIR, 'submission_lgb.csv'), index=False)\n",
        "sub_lr.to_csv(os.path.join(WORKDIR, 'submission_logistic.csv'), index=False)\n",
        "print('Saved:', os.path.join(WORKDIR, 'submission_lgb.csv'))\n",
        "print('Saved:', os.path.join(WORKDIR, 'submission_logistic.csv'))\n",
        "\n",
        "# 12. Save report CSV\n",
        "report = pd.DataFrame({\n",
        "    'model': ['LogisticRegression', 'LightGBM'],\n",
        "    'holdout_auc': [roc_auc_score(y_val, proba_lr_val), roc_auc_score(y_val, proba_lgb_val)]\n",
        "})\n",
        "report.to_csv(os.path.join(WORKDIR, 'model_report.csv'), index=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 13. Generate a 10-slide PPTX summary with visuals\n",
        "# -----------------------------\n",
        "prs = Presentation()\n",
        "prs.slide_width = Inches(13.33)  # 16:9\n",
        "prs.slide_height = Inches(7.5)\n",
        "\n",
        "def add_title_slide(prs, title, subtitle):\n",
        "    slide = prs.slides.add_slide(prs.slide_layouts[0])\n",
        "    slide.shapes.title.text = title\n",
        "    slide.placeholders[1].text = subtitle\n",
        "\n",
        "def add_text_slide(prs, title, lines):\n",
        "    slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
        "    slide.shapes.title.text = title\n",
        "    tf = slide.placeholders[1].text_frame\n",
        "    tf.clear()\n",
        "    for i, ln in enumerate(lines):\n",
        "        p = tf.add_paragraph() if i>0 else tf.paragraphs[0]\n",
        "        p.text = ln\n",
        "        p.level = 0\n",
        "\n",
        "# helper to add image slide\n",
        "def add_image_slide(prs, title, img_path, left=Inches(1), top=Inches(1), width=Inches(11)):\n",
        "    slide = prs.slides.add_slide(prs.slide_layouts[5])\n",
        "    slide.shapes.title.text = title\n",
        "    if os.path.exists(img_path):\n",
        "        slide.shapes.add_picture(img_path, left, top, width=width)\n",
        "\n",
        "# 1 Title\n",
        "add_title_slide(prs, \"Home Credit — End-to-End Baseline\", \"Logistic Regression + LightGBM\\nAuthor: (Your Name)\")\n",
        "\n",
        "# 2 Problem & Goal\n",
        "add_text_slide(prs, \"Problem & Goal\", [\n",
        "    \"Goal: Predict probability of default (TARGET) to improve credit decisions.\",\n",
        "    \"Metric: ROC-AUC (primary).\"\n",
        "])\n",
        "\n",
        "# 3 Data overview\n",
        "add_text_slide(prs, \"Data Overview\", [\n",
        "    f\"Files used: application_train/test, bureau, bureau_balance, previous_application, installments, credit_card_balance, POS_CASH.\",\n",
        "    f\"Train rows: {train.shape[0]}, Test rows: {test.shape[0]}.\"\n",
        "])\n",
        "\n",
        "# 4 EDA visual: target distribution\n",
        "add_image_slide(prs, \"Target distribution\", os.path.join(WORKDIR, 'target_distribution.png'))\n",
        "\n",
        "# 5 Missingness\n",
        "add_image_slide(prs, \"Top missing features (train)\", os.path.join(WORKDIR, 'missing_pct_top.png'))\n",
        "\n",
        "# 6 Correlation subset\n",
        "add_image_slide(prs, \"Correlation (subset)\", os.path.join(WORKDIR, 'corr_matrix_subset.png'))\n",
        "\n",
        "# 7 Modeling approach\n",
        "add_text_slide(prs, \"Modeling approach\", [\n",
        "    \"Models: Logistic Regression (explainable) and LightGBM (performance).\",\n",
        "    \"StratifiedKFold CV + RandomizedSearch for LightGBM hyperparams.\"\n",
        "])\n",
        "\n",
        "# 8 ROC comparison\n",
        "add_image_slide(prs, \"ROC comparison (holdout)\", os.path.join(WORKDIR, 'roc_comparison.png'))\n",
        "\n",
        "# 9 Feature importance\n",
        "add_image_slide(prs, \"Top feature importances\", os.path.join(WORKDIR, 'feature_importance_top20.png'))\n",
        "\n",
        "# 10 Next steps & Repo\n",
        "add_text_slide(prs, \"Next steps & Repo\", [\n",
        "    \"Next: deeper feature engineering (time-based), stacking, full k-fold training, model explainability (SHAP).\",\n",
        "    \"Repo: <PUT_YOUR_GITHUB_REPO_LINK_HERE>\",\n",
        "    \"Artifacts saved in working dir for submission and review.\"\n",
        "])\n",
        "\n",
        "pptx_path = os.path.join(WORKDIR, 'HomeCredit_presentation_with_visuals.pptx')\n",
        "prs.save(pptx_path)\n",
        "print('\\nSaved PPTX:', pptx_path)\n",
        "\n",
        "print('\\nALL DONE. Artifacts in:', WORKDIR)\n",
        "print('Files sample:', os.listdir(WORKDIR)[:50])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwUvdGge4z4I",
        "outputId": "ad95493b-5d20-4ca4-e136-3b3e4f6e196a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORKDIR: /content/home_credit_task\n",
            "Exists: application_train.csv\n",
            "Exists: application_test.csv\n",
            "Exists: bureau.csv\n",
            "Exists: bureau_balance.csv\n",
            "Exists: credit_card_balance.csv\n",
            "Exists: installments_payments.csv\n",
            "Exists: POS_CASH_balance.csv\n",
            "Exists: previous_application.csv\n",
            "Exists: HomeCredit_columns_description.csv\n",
            "Exists: sample_submission.csv\n",
            "\n",
            "Loading CSVs ...\n",
            "Shapes:\n",
            "train (307511, 122) test (48744, 121)\n",
            "bureau (1716428, 17) bureau_balance (27299925, 3)\n",
            "credit_card_balance (3840312, 23) installments (13605401, 8)\n",
            "pos_cash (10001358, 8) previous_application (1670214, 37)\n",
            "\n",
            "--- TARGET distribution ---\n",
            "TARGET\n",
            "0    0.919271\n",
            "1    0.080729\n",
            "Name: proportion, dtype: float64\n",
            "Saved plot: /content/home_credit_task/target_distribution.png\n",
            "Saved plot: /content/home_credit_task/missing_pct_top.png\n",
            "Saved plot: /content/home_credit_task/corr_matrix_subset.png\n",
            "\n",
            "Merged shapes: (307511, 26) (48744, 25)\n",
            "\n",
            "Number of features: 24\n",
            "\n",
            "=== Logistic Regression ===\n",
            "LR 5-fold AUC: 0.67660 ± 0.00183\n",
            "LR holdout AUC: 0.6779583940318442\n",
            "LR classification report (holdout, threshold=0.5):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.64      0.76     56538\n",
            "           1       0.13      0.62      0.22      4965\n",
            "\n",
            "    accuracy                           0.63     61503\n",
            "   macro avg       0.54      0.63      0.49     61503\n",
            "weighted avg       0.88      0.63      0.72     61503\n",
            "\n",
            "Saved plot: /content/home_credit_task/roc_logistic.png\n",
            "\n",
            "=== LightGBM (RandomizedSearchCV) ===\n",
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "Best LGB params: {'subsample': 0.8, 'num_leaves': 127, 'n_estimators': 400, 'min_child_samples': 50, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
            "Best LGB CV score: 0.6968492096894229\n",
            "LGB holdout AUC: 0.6996617003163785\n",
            "LGB classification report (holdout):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96     56538\n",
            "           1       0.67      0.00      0.00      4965\n",
            "\n",
            "    accuracy                           0.92     61503\n",
            "   macro avg       0.79      0.50      0.48     61503\n",
            "weighted avg       0.90      0.92      0.88     61503\n",
            "\n",
            "Saved plot: /content/home_credit_task/roc_lgb.png\n",
            "Saved plot: /content/home_credit_task/roc_comparison.png\n",
            "\n",
            "Top 20 features (LightGBM):\n",
            "DAYS_BIRTH_YEARS           3022\n",
            "AMT_CREDIT                 3018\n",
            "CREDIT_GOODS_RATIO         2990\n",
            "inst_amt_sum               2801\n",
            "INCOME_CREDIT_RATIO        2783\n",
            "bureau_credit_sum_max      2722\n",
            "inst_delay_mean            2619\n",
            "DAYS_EMPLOYED_YEARS        2589\n",
            "bureau_credit_sum_mean     2411\n",
            "prev_count                 2190\n",
            "bureau_prev_months_mean    2142\n",
            "inst_count                 2089\n",
            "cc_bal_mean                2050\n",
            "pos_count                  2048\n",
            "bureau_active_cnt          2044\n",
            "prev_amt_credit_mean       2027\n",
            "prev_amt_app_mean          1932\n",
            "AMT_INCOME_TOTAL           1757\n",
            "cc_limit_mean              1593\n",
            "pos_dpd_mean               1583\n",
            "dtype: int32\n",
            "Saved plot: /content/home_credit_task/feature_importance_top20.png\n",
            "\n",
            "Creating submissions ...\n",
            "Saved: /content/home_credit_task/submission_lgb.csv\n",
            "Saved: /content/home_credit_task/submission_logistic.csv\n",
            "\n",
            "Saved PPTX: /content/home_credit_task/HomeCredit_presentation_with_visuals.pptx\n",
            "\n",
            "ALL DONE. Artifacts in: /content/home_credit_task\n",
            "Files sample: ['target_distribution.png', 'HomeCredit_presentation_with_visuals.pptx', 'missing_pct_top.png', 'application_test.csv', 'submission_logistic.csv', 'roc_comparison.png', 'HomeCredit_columns_description.csv', 'application_train.csv', 'scaler_logistic.pkl', 'feature_importance_top20.png', 'submission_lgb.csv', 'model_report.csv', 'POS_CASH_balance.csv', 'roc_logistic.png', 'bureau_balance.csv', 'installments_payments.csv', 'model_lgb.pkl', 'sample_submission.csv', 'corr_matrix_subset.png', 'model_logistic.pkl', 'bureau.csv', 'previous_application.csv', 'roc_lgb.png', 'credit_card_balance.csv']\n"
          ]
        }
      ]
    }
  ]
}