{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfzunJtBSUhGTwdWT2BoUL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annisafitribas/ft_credit_home/blob/main/ft_credit_home.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PERSIAPAN**"
      ],
      "metadata": {
        "id": "umI9UgB3954B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import library dan workdir"
      ],
      "metadata": {
        "id": "hOvjCtnWgQqz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aMX0d0T9Ib5",
        "outputId": "471f77dd-290e-4835-c6fa-620cf65babe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORKDIR: /content/home_credit_task\n"
          ]
        }
      ],
      "source": [
        "import os, sys, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "WORKDIR = '/content/home_credit_task'\n",
        "os.makedirs(WORKDIR, exist_ok=True)\n",
        "print(\"WORKDIR:\", WORKDIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bertujuan untuk menyiapkan folder kerja dan library yang akan digunakan dalam proses pengolahan data"
      ],
      "metadata": {
        "id": "vsPmvAGV-Ndf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install dependencies if missing (Colab-friendly)"
      ],
      "metadata": {
        "id": "zg8TKacQgY_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import gdown\n",
        "except Exception:\n",
        "    !pip install -q gdown\n",
        "    import gdown\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception:\n",
        "    !pip install -q lightgbm\n",
        "    import lightgbm as lgb\n",
        "\n",
        "try:\n",
        "    from pptx import Presentation\n",
        "    from pptx.util import Inches, Pt\n",
        "except Exception:\n",
        "    !pip install -q python-pptx\n",
        "    from pptx import Presentation\n",
        "    from pptx.util import Inches, Pt\n",
        "\n",
        "try:\n",
        "    import matplotlib\n",
        "    import matplotlib.pyplot as plt\n",
        "except Exception:\n",
        "    !pip install -q matplotlib\n",
        "    import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "WK0xVTgcDaRl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sklearn / joblib"
      ],
      "metadata": {
        "id": "IdJupcjfmyUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
        "from sklearn.impute import SimpleImputer\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "rayBHCRZEIwo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Download dataset file**"
      ],
      "metadata": {
        "id": "jQZRCZGJEP8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = {\n",
        "    'application_train.csv': '1q059QolR6CNxB0PWESAjkEWIprNutajA',\n",
        "    'application_test.csv' : '1QD7ehk_hzXze0vHQuYa5qyqfDcfI8Sex',\n",
        "    'bureau.csv'           : '1hndizX1t5ab0DTnKMTedqVJ1ZxLVclhF',\n",
        "    'bureau_balance.csv'   : '1OXEQb_L6S_mZALJi4--C6RyFI6yOsq4x',\n",
        "    'credit_card_balance.csv': '1t6Hhsmj0vSCCKUlNXht_xDQ6Z6l4M0Vu',\n",
        "    'installments_payments.csv': '126xrKCW5EQrxkQoDwmN-yb00ILBKnhR8',\n",
        "    'POS_CASH_balance.csv' : '1dODAmBQLaylpM2JcCHfc4KNbbtKY7xhA',\n",
        "    'previous_application.csv': '1D4O7xf-lF_3oBeu6XMwzhpXtSvhcgoBU',\n",
        "    'HomeCredit_columns_description.csv': '1v2iGGOJjlUGSTsQz-bsjtjtyM5IQp7uW',\n",
        "    'sample_submission.csv': '1JongVA9fWMYml5XKVnbhm8TUlR5Efs0n'\n",
        "}\n",
        "\n",
        "for fname, fid in files.items():\n",
        "    dest = os.path.join(WORKDIR, fname)\n",
        "    if not os.path.exists(dest):\n",
        "        print(\"Downloading\", fname)\n",
        "        url = f\"https://drive.google.com/uc?export=download&id={fid}\"\n",
        "        gdown.download(url, dest, quiet=False)\n",
        "    else:\n",
        "        print(\"Exists:\", fname)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyUxC7z_ERaB",
        "outputId": "df1dff50-9213-454c-c219-4afe449556de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exists: application_train.csv\n",
            "Exists: application_test.csv\n",
            "Exists: bureau.csv\n",
            "Exists: bureau_balance.csv\n",
            "Exists: credit_card_balance.csv\n",
            "Exists: installments_payments.csv\n",
            "Exists: POS_CASH_balance.csv\n",
            "Exists: previous_application.csv\n",
            "Exists: HomeCredit_columns_description.csv\n",
            "Exists: sample_submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Load csv**"
      ],
      "metadata": {
        "id": "6DCJ6JlZFS3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLoading CSVs ...\")\n",
        "train = pd.read_csv(os.path.join(WORKDIR, 'application_train.csv'), low_memory=False)\n",
        "test  = pd.read_csv(os.path.join(WORKDIR, 'application_test.csv'), low_memory=False)\n",
        "bureau = pd.read_csv(os.path.join(WORKDIR, 'bureau.csv'), low_memory=False)\n",
        "bureau_balance = pd.read_csv(os.path.join(WORKDIR, 'bureau_balance.csv'), low_memory=False)\n",
        "credit_card_balance = pd.read_csv(os.path.join(WORKDIR, 'credit_card_balance.csv'), low_memory=False)\n",
        "installments = pd.read_csv(os.path.join(WORKDIR, 'installments_payments.csv'), low_memory=False)\n",
        "pos_cash = pd.read_csv(os.path.join(WORKDIR, 'POS_CASH_balance.csv'), low_memory=False)\n",
        "prev_app = pd.read_csv(os.path.join(WORKDIR, 'previous_application.csv'), low_memory=False)\n",
        "sample_sub = pd.read_csv(os.path.join(WORKDIR, 'sample_submission.csv'), low_memory=False)\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(\"train\", train.shape, \"test\", test.shape)\n",
        "print(\"bureau\", bureau.shape, \"bureau_balance\", bureau_balance.shape)\n",
        "print(\"credit_card_balance\", credit_card_balance.shape, \"installments\", installments.shape)\n",
        "print(\"pos_cash\", pos_cash.shape, \"previous_application\", prev_app.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVB95BsjFU4b",
        "outputId": "779bc12f-0f4f-468d-d097-b44c4b0f9faa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading CSVs ...\n",
            "Shapes:\n",
            "train (307511, 122) test (48744, 121)\n",
            "bureau (1716428, 17) bureau_balance (27299925, 3)\n",
            "credit_card_balance (3840312, 23) installments (13605401, 8)\n",
            "pos_cash (10001358, 8) previous_application (1670214, 37)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Quick EDA & visuals**"
      ],
      "metadata": {
        "id": "BulJn6uOFytA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTARGET distribution\")\n",
        "print(train['TARGET'].value_counts(normalize=True))\n",
        "\n",
        "# helper for saving plots\n",
        "def savefig(fig, filename):\n",
        "    path = os.path.join(WORKDIR, filename)\n",
        "    fig.savefig(path, bbox_inches='tight')\n",
        "    print('Saved plot:', path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AyuhCOzF3pw",
        "outputId": "3070373b-fa0b-4466-8c46-527310402da7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TARGET distribution\n",
            "TARGET\n",
            "0    0.919271\n",
            "1    0.080729\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 target distribution plot"
      ],
      "metadata": {
        "id": "xL2ibvHTGJb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(6,4))\n",
        "ax = fig.add_subplot(111)\n",
        "counts = train['TARGET'].value_counts().sort_index()\n",
        "ax.bar(counts.index.astype(str), counts.values)\n",
        "ax.set_title('Target distribution (counts)')\n",
        "ax.set_xlabel('TARGET')\n",
        "ax.set_ylabel('Count')\n",
        "savefig(fig, 'target_distribution.png')\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHWzu0v1GOJ5",
        "outputId": "6dd305db-c378-4522-abb6-910aa605f678"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved plot: /content/home_credit_task/target_distribution.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 top missing features (bar)"
      ],
      "metadata": {
        "id": "-ve5l0wJGSfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing = train.isna().mean().sort_values(ascending=False).head(30)\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.barh(missing.index[::-1], missing.values[::-1])\n",
        "ax.set_title('Top missing percentage (train)')\n",
        "ax.set_xlabel('Fraction missing')\n",
        "savefig(fig, 'missing_pct_top.png')\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_Y1HbtmGVXh",
        "outputId": "aadb69b6-eafa-452b-ab7e-9031ad92d24c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved plot: /content/home_credit_task/missing_pct_top.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 correlation heatmap of numeric features (sampled for speed)"
      ],
      "metadata": {
        "id": "vMhKs1a2GbDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num = train.select_dtypes(include=[np.number]).drop(['SK_ID_CURR','TARGET'], axis=1, errors='ignore')\n",
        "# sample columns to avoid huge matrix\n",
        "num_small = num.sample(n=min(30, num.shape[1]), axis=1)\n",
        "corr = num_small.corr()\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.imshow(corr.values, interpolation='nearest')\n",
        "ax.set_xticks(np.arange(len(corr.columns)))\n",
        "ax.set_xticklabels(corr.columns, rotation=90, fontsize=8)\n",
        "ax.set_yticks(np.arange(len(corr.columns)))\n",
        "ax.set_yticklabels(corr.columns, fontsize=8)\n",
        "ax.set_title('Correlation matrix (subset)')\n",
        "fig.colorbar(cax, ax=ax)\n",
        "savefig(fig, 'corr_matrix_subset.png')\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPenjorDGcTL",
        "outputId": "bfdf08d0-e75c-4de5-d511-a2bf3acd68ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved plot: /content/home_credit_task/corr_matrix_subset.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Feature engineering (same as baseline but kept clear)**"
      ],
      "metadata": {
        "id": "dtpmxTA4G0Db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Aggregations dari bureau (per SK_ID_CURR)"
      ],
      "metadata": {
        "id": "Q_-IvaliH23G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b_agg = bureau.groupby('SK_ID_CURR').agg(\n",
        "    bureau_loans_count = ('SK_ID_BUREAU', 'count'),\n",
        "    bureau_credit_sum_mean = ('AMT_CREDIT_SUM', 'mean'),\n",
        "    bureau_credit_sum_max = ('AMT_CREDIT_SUM', 'max'),\n",
        "    bureau_active_cnt = ('CREDIT_ACTIVE', lambda x: (x=='Active').sum())\n",
        ").reset_index()"
      ],
      "metadata": {
        "id": "k7yFPghbH7jV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 bureau_balance -> bad rate per bureau id then agg"
      ],
      "metadata": {
        "id": "R3kVXLfqH_i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bb_bad = bureau_balance[bureau_balance['STATUS'].isin(['2','3','4','5'])].groupby('SK_ID_BUREAU').size().rename('bad_months')\n",
        "bb_tot = bureau_balance.groupby('SK_ID_BUREAU').size().rename('total_months')\n",
        "bb = pd.concat([bb_bad, bb_tot], axis=1).fillna(0)\n",
        "bb['bad_rate'] = bb['bad_months'] / bb['total_months']\n",
        "bureau2 = bureau.merge(bb.reset_index(), on='SK_ID_BUREAU', how='left')\n",
        "b2_agg = bureau2.groupby('SK_ID_CURR').agg(\n",
        "    bureau_prev_bad_rate_mean = ('bad_rate','mean'),\n",
        "    bureau_prev_months_mean = ('total_months','mean')\n",
        ").reset_index()\n",
        "\n",
        "b_agg = b_agg.merge(b2_agg, on='SK_ID_CURR', how='left')"
      ],
      "metadata": {
        "id": "O93EIi6jH94U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 previous_application aggregates"
      ],
      "metadata": {
        "id": "CnKtbJ-iIERM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prev_agg = prev_app.groupby('SK_ID_CURR').agg(\n",
        "    prev_count = ('SK_ID_PREV','count'),\n",
        "    prev_amt_app_mean = ('AMT_APPLICATION','mean'),\n",
        "    prev_amt_credit_mean = ('AMT_CREDIT','mean'),\n",
        "    prev_approved = ('NAME_CONTRACT_STATUS', lambda x: (x=='Approved').sum())\n",
        ").reset_index()"
      ],
      "metadata": {
        "id": "6SoR84nKILR5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 installments"
      ],
      "metadata": {
        "id": "hBX_nHR8IOmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inst_agg = installments.groupby('SK_ID_CURR').agg(\n",
        "    inst_count = ('NUM_INSTALMENT_VERSION','count'),\n",
        "    inst_amt_sum = ('AMT_PAYMENT','sum'),\n",
        "    inst_delay_mean = ('DAYS_ENTRY_PAYMENT', lambda x: np.nanmean(x - installments.loc[x.index,'DAYS_INSTALMENT']))\n",
        ").reset_index()"
      ],
      "metadata": {
        "id": "Gzqa_BwRITGP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 credit_card & pos"
      ],
      "metadata": {
        "id": "yAqhyu60Icll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cc_agg = credit_card_balance.groupby('SK_ID_CURR').agg(\n",
        "    cc_count = ('SK_ID_PREV','count'),\n",
        "    cc_bal_mean = ('AMT_BALANCE','mean'),\n",
        "    cc_limit_mean = ('AMT_CREDIT_LIMIT_ACTUAL','mean')\n",
        ").reset_index()\n",
        "\n",
        "pos_agg = pos_cash.groupby('SK_ID_CURR').agg(\n",
        "    pos_count = ('SK_ID_PREV','count'),\n",
        "    pos_dpd_mean = ('SK_DPD','mean')\n",
        ").reset_index()"
      ],
      "metadata": {
        "id": "1V8pPrw3Ia5O"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6 application-level features"
      ],
      "metadata": {
        "id": "4VIG_pLfIigS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_app_features(df):\n",
        "    df = df.copy()\n",
        "    df['DAYS_BIRTH_YEARS'] = (-df['DAYS_BIRTH']) / 365.25\n",
        "    df['DAYS_EMPLOYED_YEARS'] = df['DAYS_EMPLOYED'].replace(365243, np.nan) / -365.25\n",
        "    df['INCOME_CREDIT_RATIO'] = df['AMT_INCOME_TOTAL'] / (df['AMT_CREDIT'] + 1)\n",
        "    df['CREDIT_GOODS_RATIO'] = df['AMT_CREDIT'] / (df['AMT_GOODS_PRICE'] + 1)\n",
        "    return df[['SK_ID_CURR','DAYS_BIRTH_YEARS','DAYS_EMPLOYED_YEARS','AMT_INCOME_TOTAL','AMT_CREDIT','INCOME_CREDIT_RATIO','CREDIT_GOODS_RATIO']]\n",
        "\n",
        "app_train_feats = make_app_features(train)\n",
        "app_test_feats = make_app_features(test)"
      ],
      "metadata": {
        "id": "q70a3-F8IghU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Merge features**"
      ],
      "metadata": {
        "id": "qTD0bxa7I3hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_base = train[['SK_ID_CURR','TARGET']].merge(b_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(prev_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(inst_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(cc_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(pos_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(app_train_feats, on='SK_ID_CURR', how='left')\n",
        "\n",
        "test_base = test[['SK_ID_CURR']].merge(b_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(prev_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(inst_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(cc_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(pos_agg, on='SK_ID_CURR', how='left') \\\n",
        "                               .merge(app_test_feats, on='SK_ID_CURR', how='left')"
      ],
      "metadata": {
        "id": "KY2_5jmUI8JL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Prepare X, y; preprocessing**"
      ],
      "metadata": {
        "id": "TGUGA-xCJZJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = train_base['TARGET']\n",
        "X = train_base.drop(['SK_ID_CURR','TARGET'], axis=1)\n",
        "X_test = test_base.drop(['SK_ID_CURR'], axis=1)\n",
        "\n",
        "# numeric / categorical separation\n",
        "num_cols = [c for c in X.columns if X[c].dtype.kind in 'biufc']\n",
        "cat_cols = [c for c in X.columns if c not in num_cols]\n",
        "\n",
        "# Impute numeric with median (computed from train)\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
        "X_test[num_cols] = num_imputer.transform(X_test[num_cols])\n",
        "\n",
        "# For categorical (if any), fill and label-encode simple\n",
        "for c in cat_cols:\n",
        "    X[c] = X[c].fillna('MISSING').astype(str)\n",
        "    X_test[c] = X_test[c].fillna('MISSING').astype(str)\n",
        "for c in cat_cols:\n",
        "    X[c], _ = pd.factorize(X[c])\n",
        "    X_test[c], _ = pd.factorize(X_test[c])\n",
        "\n",
        "# Ensure X_test has all columns (reindex)\n",
        "X_test = X_test.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "print(\"\\nNumber of features:\", X.shape[1])"
      ],
      "metadata": {
        "id": "58N4AXNzJcOA",
        "outputId": "41dd7ad5-7516-4815-b32b-c92f594ac0c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of features: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Train/validation split and scaling**"
      ],
      "metadata": {
        "id": "1yBLSC9OJ4Ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr, X_val, y_tr, y_val = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
        "scaler = StandardScaler()\n",
        "X_tr_scaled = scaler.fit_transform(X_tr)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "YlQkb_G0J8eW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Logistic Regression**"
      ],
      "metadata": {
        "id": "F3iqq3r3KA3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Logistic Regression ===\")\n",
        "lr = LogisticRegression(max_iter=2000, class_weight='balanced', n_jobs=-1)\n",
        "# 5-fold CV\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "lr_cv_scores = []\n",
        "for tr_idx, vc_idx in kf.split(X, Y):\n",
        "    lr.fit(scaler.fit_transform(X.iloc[tr_idx]), Y.iloc[tr_idx])\n",
        "    p = lr.predict_proba(scaler.transform(X.iloc[vc_idx]))[:,1]\n",
        "    lr_cv_scores.append(roc_auc_score(Y.iloc[vc_idx], p))\n",
        "print(\"LR 5-fold AUC: %.5f ± %.5f\" % (np.mean(lr_cv_scores), np.std(lr_cv_scores)))\n",
        "\n",
        "# fit on training partition and validate\n",
        "lr.fit(X_tr_scaled, y_tr)\n",
        "proba_lr_val = lr.predict_proba(X_val_scaled)[:,1]\n",
        "print(\"LR holdout AUC:\", roc_auc_score(y_val, proba_lr_val))\n",
        "print(\"LR classification report (holdout, threshold=0.5):\")\n",
        "print(classification_report(y_val, (proba_lr_val>0.5).astype(int)))\n",
        "\n",
        "# save logistic model & scaler\n",
        "joblib.dump(lr, os.path.join(WORKDIR, 'model_logistic.pkl'))\n",
        "joblib.dump(scaler, os.path.join(WORKDIR, 'scaler_logistic.pkl'))\n",
        "\n",
        "# ROC curve plot (LR)\n",
        "fpr, tpr, _ = roc_curve(y_val, proba_lr_val)\n",
        "roc_auc_lr = auc(fpr, tpr)\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(fpr, tpr)\n",
        "ax.plot([0,1],[0,1], linestyle='--')\n",
        "ax.set_title(f'Logistic ROC (AUC={roc_auc_lr:.4f})')\n",
        "ax.set_xlabel('FPR')\n",
        "ax.set_ylabel('TPR')\n",
        "savefig(fig, 'roc_logistic.png')\n",
        "plt.close(fig)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9AW9EK9KEBp",
        "outputId": "4871cfe3-b1f9-47f0-f37f-9a9f6dcfea1a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression ===\n",
            "LR 5-fold AUC: 0.67660 ± 0.00183\n",
            "LR holdout AUC: 0.6779583940318442\n",
            "LR classification report (holdout, threshold=0.5):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.64      0.76     56538\n",
            "           1       0.13      0.62      0.22      4965\n",
            "\n",
            "    accuracy                           0.63     61503\n",
            "   macro avg       0.54      0.63      0.49     61503\n",
            "weighted avg       0.88      0.63      0.72     61503\n",
            "\n",
            "Saved plot: /content/home_credit_task/roc_logistic.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. LightGBM**"
      ],
      "metadata": {
        "id": "UmRclqBGXWyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLightGBM\")\n",
        "\n",
        "best_lgb = lgb.LGBMClassifier(\n",
        "    objective='binary',\n",
        "    random_state=42,\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=400,\n",
        "    num_leaves=63,\n",
        "    min_child_samples=20,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "best_lgb.fit(X_tr, y_tr)\n",
        "proba_lgb_val = best_lgb.predict_proba(X_val)[:,1]\n",
        "\n",
        "print(\"LightGBM FAST holdout AUC:\", roc_auc_score(y_val, proba_lgb_val))\n",
        "print(\"LGB classification report (holdout):\")\n",
        "print(classification_report(y_val, (proba_lgb_val > 0.5).astype(int)))\n",
        "\n",
        "# Save model\n",
        "joblib.dump(best_lgb, os.path.join(WORKDIR, 'model_lgb.pkl'))\n"
      ],
      "metadata": {
        "id": "IoVJWsAdqEzd",
        "outputId": "ae9389ae-dd2c-4d27-9394-8e864bb998e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LightGBM\n",
            "[LightGBM] [Info] Number of positive: 19860, number of negative: 226148\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194371 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 5000\n",
            "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 24\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432482\n",
            "[LightGBM] [Info] Start training from score -2.432482\n",
            "LightGBM FAST holdout AUC: 0.6991342168535724\n",
            "LGB classification report (holdout):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96     56538\n",
            "           1       0.42      0.00      0.01      4965\n",
            "\n",
            "    accuracy                           0.92     61503\n",
            "   macro avg       0.67      0.50      0.48     61503\n",
            "weighted avg       0.88      0.92      0.88     61503\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/home_credit_task/model_lgb.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "muXp5J_mqIWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. Feature importance (LGB) and plot**"
      ],
      "metadata": {
        "id": "mx3bqWJbXs9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fi = pd.Series(best_lgb.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "print('\\nTop 20 features (LightGBM):')\n",
        "print(fi.head(20))\n",
        "\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "ax = fig.add_subplot(111)\n",
        "topn = fi.head(20)[::-1]\n",
        "ax.barh(topn.index, topn.values)\n",
        "ax.set_title('Top 20 feature importances (LightGBM)')\n",
        "ax.set_xlabel('Importance')\n",
        "savefig(fig, 'feature_importance_top20.png')\n",
        "plt.close(fig)\n"
      ],
      "metadata": {
        "id": "AO2Dkm-5Xw4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **11. Create submissions**"
      ],
      "metadata": {
        "id": "J3w4YCNbX8YC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nCreating submissions ...')\n",
        "preds_lg_test = best_lgb.predict_proba(X_test)[:,1]\n",
        "preds_lr_test = lr.predict_proba(X_test_scaled)[:,1]\n",
        "\n",
        "sub_lgb = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': preds_lg_test})\n",
        "sub_lr  = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': preds_lr_test})\n",
        "\n",
        "sub_lgb.to_csv(os.path.join(WORKDIR, 'submission_lgb.csv'), index=False)\n",
        "sub_lr.to_csv(os.path.join(WORKDIR, 'submission_logistic.csv'), index=False)\n",
        "print('Saved:', os.path.join(WORKDIR, 'submission_lgb.csv'))\n",
        "print('Saved:', os.path.join(WORKDIR, 'submission_logistic.csv'))"
      ],
      "metadata": {
        "id": "6-KT3XLjYTUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **12. Save report CSV**"
      ],
      "metadata": {
        "id": "QXlVbu8AYaPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report = pd.DataFrame({\n",
        "    'model': ['LogisticRegression', 'LightGBM'],\n",
        "    'holdout_auc': [roc_auc_score(y_val, proba_lr_val), roc_auc_score(y_val, proba_lgb_val)]\n",
        "})\n",
        "report.to_csv(os.path.join(WORKDIR, 'model_report.csv'), index=False)"
      ],
      "metadata": {
        "id": "Kyy4vl9PYeCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **13. Visualisasi**"
      ],
      "metadata": {
        "id": "iEA0a7s0evrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "print(\"\\nSemua visualisasi sudah dibuat dan disimpan di folder:\")\n",
        "print(WORKDIR)\n",
        "\n",
        "visuals = [\n",
        "    'target_distribution.png',\n",
        "    'missing_pct_top.png',\n",
        "    'corr_matrix_subset.png',\n",
        "    'roc_lr.png',\n",
        "    'roc_lgbm.png',\n",
        "    'feature_importance_lgbm.png'\n",
        "]\n",
        "\n",
        "for v in visuals:\n",
        "    img_path = os.path.join(WORKDIR, v)\n",
        "    if os.path.exists(img_path):\n",
        "        print(f\"\\n▶ {v}\")\n",
        "        display(Image(img_path))\n",
        "    else:\n",
        "        print(f\"\\n⚠️ File tidak ditemukan: {v}\")\n",
        "\n",
        "print(\"\\nSemua plot siap dipakai untuk PPT atau laporan.\")"
      ],
      "metadata": {
        "id": "GOv-fbw8ezn5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}